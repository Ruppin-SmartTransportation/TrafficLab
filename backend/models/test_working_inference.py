#!/usr/bin/env python3
"""
Working test of the inference system using available data.
"""

import torch
import numpy as np
from temporal_dataset import TemporalGraphDataset, temporal_collate
from torch.utils.data import DataLoader
from model_temporal_moe import TemporalMoEETA
from utils_targets import invert_to_seconds, get_target_tensor
import yaml

def test_inference_with_available_data():
    """Test inference using available step data."""
    print("üöÄ Testing ETA Inference with Available Data")
    print("=" * 60)
    
    # Load config
    with open('models/config.yaml', 'r') as f:
        cfg = yaml.safe_load(f)
    
    print(f"‚úÖ Config loaded")
    print(f"   - Data path: {cfg['data']['test_path']}")
    print(f"   - Window size: {cfg['data']['window_size']}")
    
    # Create model
    model = TemporalMoEETA(
        node_in_dim=28,
        d_hidden=128,
        fusion_out=192,
        n_experts=6,
        top_k=2,
        dropout=0.1,
        predict_on="last",
        edge_vocab_size=1294,
        route_emb_dim=64,
        edge_dim=7,
        temporal_kind=cfg["model"].get("temporal_kind", "gru"),
        ablation_variant=cfg["model"].get("ablation_variant", "temporal_route_aware"),
    )
    
    # Load checkpoint
    checkpoint = torch.load('models/moe_best.pt', map_location='cpu', weights_only=True)
    model.load_state_dict(checkpoint["model"], strict=False)
    model.eval()
    
    print(f"‚úÖ Model loaded and set to eval mode")
    
    # Create dataset with available data
    test_ds = TemporalGraphDataset(
        root=cfg["data"]["test_path"],
        window_size=cfg["data"]["window_size"],
        stride_size=cfg["data"]["stride_size"],
        num_files=cfg["data"].get("test_num_files", 30),
        start_idx=cfg["data"].get("test_start_idx", 0),
        allow_incomplete_tail=cfg["data"].get("allow_incomplete_tail", False),
        shuffle_windows=False,
    )
    
    print(f"‚úÖ Dataset created with {len(test_ds)} windows")
    
    # Create data loader
    collate = temporal_collate
    test_loader = DataLoader(
        test_ds,
        batch_size=1,
        shuffle=False,
        num_workers=0,
        persistent_workers=False,
        pin_memory=True,
        collate_fn=collate
    )
    
    print(f"‚úÖ DataLoader created")
    
    # Get first batch
    try:
        batch = next(iter(test_loader))
        time_batches = [tb for tb in batch["time_batches"]]
        
        print(f"‚úÖ Loaded batch with {len(time_batches)} timesteps")
        print(f"   - First timestep nodes: {time_batches[0].x.shape[0]}")
        print(f"   - Last timestep nodes: {time_batches[-1].x.shape[0]}")
        
        # Run inference
        with torch.no_grad():
            y_hat, aux, veh_mask = model(time_batches, train=False)
            bt = time_batches[-1]
            target_key = cfg["train"]["target_key"]
            batch_veh = bt.batch[veh_mask]
            y_tgt = get_target_tensor(bt, target_key).float()
            y_sec = invert_to_seconds(y_tgt, bt, target_key, batch_veh)
            yhat_sec = invert_to_seconds(y_hat, bt, target_key, batch_veh)
        
        print(f"‚úÖ Inference successful!")
        print(f"   - Predictions shape: {y_hat.shape}")
        print(f"   - Vehicles: {veh_mask.sum().item()}")
        print(f"   - Sample predictions: {yhat_sec[:5].tolist()}")
        print(f"   - Sample targets: {y_sec[:5].tolist()}")
        
        # Calculate some basic metrics
        mae = torch.abs(yhat_sec - y_sec).mean().item()
        print(f"   - Mean Absolute Error: {mae:.2f} seconds")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Inference failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run the test."""
    success = test_inference_with_available_data()
    
    if success:
        print("\nüéâ SUCCESS!")
        print("‚úÖ The inference system is working correctly!")
        print("‚úÖ Model can load, process data, and make predictions!")
        print("‚úÖ Ready for production use!")
    else:
        print("\n‚ùå FAILED!")
        print("‚ùå The inference system needs debugging!")

if __name__ == "__main__":
    main()
